# -*- coding: utf-8 -*-
"""Social Media Sentimental  Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WwW3I2FxEhstzhPm0AdNqT--iL-ibQJv

**Importing Modules**
"""

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# ü™£ Step 1: Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

"""**Reading CSV File in the Data Frame**"""

df = pd.read_csv('/content/Social Media Sentimental Analysis.csv')

print(df.to_string())

import pandas as pd

# Ensure df is defined (re-read if necessary due to kernel state issues)
if 'df' not in locals() and 'df' not in globals():
    df = pd.read_csv('/content/Social Media Sentimental Analysis.csv')

print(df.info())

"""**Display First Five Rows of the Dataset**"""

df.head()

"""**Total names of the column**"""

print(df.columns)

print(df.head())        # ‚úÖ shows first 5 rows
print(df.sample(5))     # ‚úÖ shows random 5 rows

"""**To Get Numeric + Categorical Summaries**"""

df.describe(include='all')

"""**Visualize Missing Values using Heat Maps**"""

sns.heatmap(df.isnull(), cbar=False)

"""**text preprocessing in NLP (Natural Language Processing).**"""

import re
import nltk
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt')

from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

"""**initial setup phase of your NLP (Natural Language Processing) pipeline which converts messy social media posts (tweets, comments, etc.) into clean, meaningful text**"""

STOPWORDS = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def clean_text(text):
    # 1Ô∏è‚É£ Convert to lowercase
    text = text.lower()

    # 2Ô∏è‚É£ Remove URLs
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)

    # 3Ô∏è‚É£ Remove mentions and hashtags
    text = re.sub(r'@\w+|#\w+', '', text)

    # 4Ô∏è‚É£ Remove punctuation and special characters
    text = re.sub(r'[^a-z\s]', '', text)

    # 5Ô∏è‚É£ Tokenize
    words = nltk.word_tokenize(text)

    # 6Ô∏è‚É£ Remove stopwords and lemmatize
    cleaned_words = [lemmatizer.lemmatize(w) for w in words if w not in STOPWORDS]

    # 7Ô∏è‚É£ Join words back
    return " ".join(cleaned_words)

import pandas as pd
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

# Ensure nltk resources are downloaded
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt')
nltk.download('punkt_tab') # Explicitly download punkt_tab

STOPWORDS = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def clean_text(text):
    # Handle non-string input, e.g., NaN
    if not isinstance(text, str):
        return ''
    text = text.lower()
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
    text = re.sub(r'@\w+|#\w+', '', text)
    text = re.sub(r'[^a-z\s]', '', text)
    words = nltk.word_tokenize(text)
    cleaned_words = [lemmatizer.lemmatize(w) for w in words if w not in STOPWORDS]
    return " ".join(cleaned_words)

# Ensure df is defined before trying to access it
if 'df' not in locals() and 'df' not in globals():
    df = pd.read_csv('/content/Social Media Sentimental Analysis.csv')

df['Clean_Text'] = df['Text'].apply(clean_text)
display(df[['Text', 'Clean_Text']].sample(5))

"""**nltk Resource Downloads**"""

import nltk
nltk.download('punkt_tab')

"""**It shows 5 random examples from your dataset**"""

# Ensure df is defined (re-read if necessary due to kernel state issues)
# This is added to address persistent NameError for df
if 'df' not in locals() and 'df' not in globals():
    import pandas as pd
    df = pd.read_csv('/content/Social Media Sentimental Analysis.csv')

# Ensure Clean_Text column exists before attempting to display
# This might be needed if the previous cell defining 'Clean_Text' was skipped or failed
if 'Clean_Text' not in df.columns:
    import re
    import nltk
    from nltk.corpus import stopwords
    from nltk.stem import WordNetLemmatizer

    # Ensure nltk resources are downloaded
    nltk.download('stopwords')
    nltk.download('wordnet')
    nltk.download('punkt')
    nltk.download('punkt_tab') # Explicitly download punkt_tab

    STOPWORDS = set(stopwords.words('english'))
    lemmatizer = WordNetLemmatizer()

    def clean_text(text):
        if not isinstance(text, str):
            return ''
        text = text.lower()
        text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
        text = re.sub(r'@\w+|#\w+', '', text)
        text = re.sub(r'[^a-z\s]', '', text)
        words = nltk.word_tokenize(text)
        cleaned_words = [lemmatizer.lemmatize(w) for w in words if w not in STOPWORDS]
        return " ".join(cleaned_words)

    df['Clean_Text'] = df['Text'].apply(clean_text)

display(df[['Text', 'Clean_Text']].sample(5))

"""**Sentiment Analysis using TF-IDF and Logistic Regression**"""

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
import pandas as pd # Ensure pandas is imported

# Ensure df is defined
if 'df' not in locals() and 'df' not in globals():
    df = pd.read_csv('/content/Social Media Sentimental Analysis.csv')

# Convert 'Sentiment' to lowercase and filter for binary classification
df_filtered = df[df['Sentiment'].str.strip().str.lower().isin(['positive', 'negative'])].copy()

X = df_filtered['Text'].values
y = df_filtered['Sentiment'].str.strip().str.lower().map({'positive': 1, 'negative': 0}).values

# Split the filtered data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

vect = TfidfVectorizer(max_features=10000, ngram_range=(1,2))
Xtr = vect.fit_transform(X_train)
Xte = vect.transform(X_test)

clf = LogisticRegression(max_iter=1000)
clf.fit(Xtr, y_train)

print("‚úÖ Training done successfully!")

"""**Finding Sentiment values**"""

import pandas as pd # Ensure pandas is imported

# Ensure df is defined
if 'df' not in locals() and 'df' not in globals():
    df = pd.read_csv('/content/Social Media Sentimental Analysis.csv')

y = df['Sentiment'].values

print(y)

print(y[:10])

"""**Model Evaluation (Accuracy and Classification Report)**"""

from sklearn.metrics import accuracy_score, classification_report
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression

# Check if clf and related variables are defined. If not, re-run the setup.
if 'clf' not in globals() or 'Xte' not in globals() or 'y_test' not in globals():
    print("‚ö†Ô∏è Re-running model setup due to missing variables (e.g., after kernel restart).")

    # Ensure df is defined (re-read if necessary)
    if 'df' not in locals() and 'df' not in globals():
        df = pd.read_csv('/content/Social Media Sentimental Analysis.csv')

    # Filter for binary classification
    df_filtered = df[df['Sentiment'].str.strip().str.lower().isin(['positive', 'negative'])].copy()

    X = df_filtered['Text'].values
    y = df_filtered['Sentiment'].str.strip().str.lower().map({'positive': 1, 'negative': 0}).values

    # Split the filtered data
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    vect = TfidfVectorizer(max_features=10000, ngram_range=(1,2))
    Xtr = vect.fit_transform(X_train)
    Xte = vect.transform(X_test)

    clf = LogisticRegression(max_iter=1000)
    clf.fit(Xtr, y_train)
    print("‚úÖ Model and vectorizer re-initialized and re-trained.")


# Get predictions from the trained model
preds = clf.predict(Xte)

print("üéØ Accuracy:", accuracy_score(y_test, preds))
print("\nüìä Classification Report:\n", classification_report(y_test, preds))

"""**Applying Confusion Matrix**"""

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import seaborn as sns

# Step 1: Get predictions from the trained model
preds = clf.predict(Xte)

# Step 2: Generate confusion matrix
cm = confusion_matrix(y_test, preds)

# Step 3: Visualize confusion matrix using Seaborn
plt.figure(figsize=(6,4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])
plt.title('üß© Confusion Matrix for Sentiment Classification')
plt.xlabel('Predicted Labels')
plt.ylabel('Actual Labels')
plt.show()

# Optional: Display accuracy
from sklearn.metrics import accuracy_score
print("üéØ Accuracy:", accuracy_score(y_test, preds))

"""**Saving Model For Future use**"""

new_texts = [
    "I love this song, it‚Äôs amazing!",
    "This movie was terrible and boring.",
    "The food was okay, not great but not bad."
]

# Transform text using same TF-IDF vectorizer
new_text_tfidf = vect.transform(new_texts)

# Predict
predictions = clf.predict(new_text_tfidf)

# Display results
for text, pred in zip(new_texts, predictions):
    label = "Positive üòä" if pred == 1 else "Negative üòû"
    print(f"Text: {text}\nPredicted Sentiment: {label}\n")

import joblib

# Save model and vectorizer
joblib.dump(clf, 'sentiment_model.pkl')
joblib.dump(vect, 'tfidf_vectorizer.pkl')

print("üóÇÔ∏è Model and vectorizer saved successfully!")

"""**Applying Statistical Techniques**"""

mean_likes = df['Likes'].mean()
median_likes = df['Likes'].median()
mode_likes = df['Likes'].mode()[0]  # mode() returns a Series, so take the first value

print("Mean Likes:", mean_likes)
print("Median Likes:", median_likes)
print("Mode Likes:", mode_likes)

print("üìä Mean values:\n", df.mean(numeric_only=True))
print("\nüìä Median values:\n", df.median(numeric_only=True))
print("\nüìä Mode values:\n", df.mode(numeric_only=True).iloc[0])

"""**Finding Standard Deviation of all the Columnns**"""

print("üìä Standard Deviation of all numeric columns:")
print(df.std(numeric_only=True))

"""**Finding Variance of all numeric columns**"""

print("üìä Variance of all numeric columns:")
print(df.var(numeric_only=True))

"""**Bar Graph (mean of numeric columns)**"""

plt.figure(figsize=(10,6))
df.mean(numeric_only=True).plot(kind='bar', color='skyblue')
plt.title('üìä Mean of Numeric Columns')
plt.xlabel('Columns')
plt.ylabel('Mean Value')
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

"""**Histogram (distribution of numeric columns)**"""

df.hist(figsize=(12,8), bins=20, color='lightgreen', edgecolor='black')
plt.subtitle('üìà Histogram of Numeric Columns')
plt.show()

"""**Boxplot(spread and outliers)**"""

plt.figure(figsize=(10,6))
sns.boxplot(data=df.select_dtypes(include='number'), palette='Set2')
plt.title('üì¶ Boxplot of Numeric Columns')
plt.show()

"""**Heatmap (correlation between numeric columns)**"""

plt.figure(figsize=(8,6))
sns.heatmap(df.corr(numeric_only=True), annot=True, cmap='coolwarm', fmt=".2f")
plt.title('üî• Correlation Heatmap')
plt.show()

"""**overall word count using Word Cloud Visualization**"""

from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Combine all cleaned text
all_words = ' '.join(df['Clean_Text'])

# Create a WordCloud object
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_words)

# Display
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title("Overall Word Cloud", fontsize=16)
plt.show()

# Ensure df is defined (re-read if necessary due to kernel state issues)
# This is added to address persistent NameError for df
if 'df' not in locals() and 'df' not in globals():
    import pandas as pd
    df = pd.read_csv('/content/Social Media Sentimental Analysis.csv')

# Ensure Clean_Text column exists before attempting to use it
# This might be needed if the previous cell defining 'Clean_Text' was skipped or failed
if 'Clean_Text' not in df.columns:
    import re
    import nltk
    from nltk.corpus import stopwords
    from nltk.stem import WordNetLemmatizer

    # Ensure nltk resources are downloaded
    nltk.download('stopwords')
    nltk.download('wordnet')
    nltk.download('punkt')
    nltk.download('punkt_tab') # Explicitly download punkt_tab

    STOPWORDS = set(stopwords.words('english'))
    lemmatizer = WordNetLemmatizer()

    def clean_text(text):
        if not isinstance(text, str):
            return ''
        text = text.lower()
        text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
        text = re.sub(r'@\w+|#\w+', '', text)
        text = re.sub(r'[^a-z\s]', '', text)
        words = nltk.word_tokenize(text)
        cleaned_words = [lemmatizer.lemmatize(w) for w in words if w not in STOPWORDS]
        return " ".join(cleaned_words)

    df['Clean_Text'] = df['Text'].apply(clean_text)

positive_text = ' '.join(df[df['Sentiment'].str.strip().str.lower() == 'positive']['Clean_Text'])
negative_text = ' '.join(df[df['Sentiment'].str.strip().str.lower() == 'negative']['Clean_Text'])

"""**Sentiment Distribution Plot Shows how many posts fall into each sentiment category (Positive, Negative, Neutral).
It‚Äôs useful for checking data balance before training a model.**
"""

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(6,4))
sns.countplot(x='Sentiment', data=df, palette='viridis')
plt.title('Sentiment Distribution')
plt.xlabel('Sentiment')
plt.ylabel('Number of Posts')
plt.show()